{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 - Place Recognition\n",
    "\n",
    "In this chapter we will dive into the field of place recognition. In the context of visual odometry place recognition is importent for recognizin when arriving at a already visited location so that loops can be closed. In order to be able to efficiently recognize locations we need a efficient representation of the features of a imgae whicih then can be clustered and seaeched for similar ones. Obviously place recognition can also be used for other application for example image retrieval where we want to find similar images to a given one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show how imprtant a efficient representation of a image is for image retrieval. Lets say we have a database with 100 million images. How can we search through all of those images to find a matching one in about 6 secounds?\n",
    "\n",
    "![Image Querying](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_14/1_image_query.png)\n",
    "*Figure 1: Image Querying. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/12b_recognition.pdf)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our general goal is to query a database with $N$ images. If we extract $M$ features from each image then if we compare our query image with all the others we have iterate over each image ($O(N)$) and then compare each feature of our query image with each feature of the image from the database ($O(M^2)$). This means that the total complexity is $N \\cdot M^2$. T get an impression of this lets assume we have 100 million images in the database and we extract 1'000 features from each image. Then we would need to run 100'000'000'000'000 feature comparison. As a reference if each comparison takes 0.1 ms, then the image query would take 317 years!\n",
    "\n",
    "So how can we reduce this complexity? Well the answer is - inverted file index - we need to use a representation where we only iterarte over the features and not the images like this we reduce the complexity to $O(M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing local features\n",
    "\n",
    "Indexing of visual features works similar to the index of a text document where most often at the end of the book there is a list listing which word appears on which page. For the visual context we want to finde the images that contain a certain feature. However compared to the example with text for image feature we have infinite different features whereas for text we most often have at least a boundend number of words. To reduce the number we can define Visual words as well a a vocabulay of such visual words. This approach is called **Bag if Words (BOW)**.\n",
    "\n",
    "How can we extract such visual words from the feature descriptors? To get a reasonable vocabulary we need plenty of data and therefora a large enought dataset. Thwn for each image the features and descripors are extracted and mapped into the descriptor space which depends on the representation of the descriptor. Then the descriptor space is clustered into $K$ clustes where $K$ is the number of visual words we want to have. For each cluster the centerpoint (centroid) is derived and represents the visual word. Usually the centroid is derived by thaking the arithmetic average of all desriptors in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
