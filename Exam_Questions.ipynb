{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application questions\n",
    "\n",
    "**1. Summarize the building blocks of a visual odometry (VO) or visual SLAM (VSLAM) algorithm**\n",
    "\n",
    "- Image capturing (sequential)\n",
    "- Feature detection & decription (SIFT)\n",
    "- Feature matching (description matching e.g SSD (robust error minimizing))\n",
    "- motion estimation (2D-2D (8-point RANSAC) then 3D-2D (P3+1P) (3D-3D (minimizing euclidean distance))\n",
    "- local optmization (bundle adjustment & pose graph opt.)\n",
    "- global optimization (SLAM) loop closing\n",
    "\n",
    "**2. Augmented reality (AR) is a view of a physical scene augmented by computer-generated sensory\n",
    "inputs, such as data or graphics. Suppose you want to design an augmented reality system that\n",
    "super-imposes text labels to the image of real physical objects. Summarize the building blocks of\n",
    "an AR algorithm.**\n",
    "\n",
    "- VO for getting pose of camera\n",
    "- 3D pointcloud \n",
    "- surface estimation (dense reconstruction if accuracy needed)\n",
    "- (AI for recognizing object and giving name)\n",
    "- Add label in 3D (anchor to a point in the existing cloud)\n",
    "- backproject label as mesh onto the imagte plain.\n",
    "- display \n",
    "\n",
    "**3. Suppose that your task is to reconstruct an object from different views. How do you proceed?**\n",
    "- **Hierarchical SFM**\n",
    "- Feature detection & decription (SIFT)\n",
    "- Feature matching (description matching e.g SSD (robust error minimizing))\n",
    "- cluster images in to groups of 3 images with same features\n",
    "- take two of these images to construct pointcloud (8 point algo.)\n",
    "- merge 3rd view (3-point RANSAC) & BA\n",
    "- merge clusters pairwise (3D-3D) & BA\n",
    "\n",
    "**4. Building a panorama stitching application. Summarize the building blocks.**\n",
    "- Feature detection & decription (SIFT) (rotation invariant)\n",
    "- Feature matching (description matching e.g SSD (robust error minimizing)) \n",
    "- warp 2nd image to minimize euclidian distance with respect to feature location in first image\n",
    "- for good results rotation of camera arround one single point then roto-(trans)lations warp performs perfect :)\n",
    "\n",
    "**5. How would you design a mobile tourist app? The user points the phone in the direction of a\n",
    "landmark and the app displays tag with the name of it. How would you implement it?**\n",
    "- google API :)\n",
    "- landmark recognition\n",
    "- DB of images\n",
    "- Bag Of Words\n",
    "\n",
    "\n",
    "**6. Assume that we have several images downloaded from flicker showing the two towers of\n",
    "Grossmünster. Since such images were uploaded by different persons they will have different\n",
    "camera parameters (intrinsic and extrinsic), different lighting, different resolutions and so on. If\n",
    "you were supposed to create a 3D model of Grossmünster, what kind of approach would you use?\n",
    "Can you get a dense 3D model or it will be a sparse one? Please explain the pipeline that you\n",
    "propose for this scenario.**\n",
    "- hierarchical SFM\n",
    "- 8-point but with Fundamental matrix instead of essential matrix\n",
    "- only really solvable if same focal length (only cluster images with same f (info on flicker))\n",
    "- for cluster mergeing we can merge from different f\n",
    "- zero meaning\n",
    "- use SIFT for features -> spares one.\n",
    "- when normalizing for f then we can also make dense reconstruction\n",
    "\n",
    "\n",
    "**7. Assume that you move around a statue with a camera and take pictures in a way that the statue is\n",
    "not far from the camera and always completely visible in the image. If you were supposed to find\n",
    "out where the pictures were taken, what would you do with the images? What kind of approach\n",
    "would you use? Since the camera motion is around the statue, the images contain different parts\n",
    "of the statue. How do you deal with this problem?**\n",
    "- small movements between images\n",
    "\n",
    "\n",
    "**8. Suppose that you have two robots exploring an environment, explain how the robots should\n",
    "localize themselves and each other with respect to the environment? What are the alternative\n",
    "solutions?**\n",
    "- same bootstrap results in same world frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical questions\n",
    "\n",
    "## 01 – Introduction\n",
    "\n",
    "\n",
    "**1. Provide a definition of Visual Odometry.**\n",
    "- VO is the process of incrementally estimating the pose of the vehicle by examining the changes that motion induces on the images of its onboard cameras \n",
    "\n",
    "**2. Explain the most important differences between VO, VSLAM and SFM.**\n",
    "- VO focuses on estimating the 6DoF motion of the camera **sequentially** (as a new frame arrives) and in real time.\n",
    "- SFM is more general than VO and tackles the problem of 3D reconstruction and 6DOF pose estimation from **unordered image sets**\n",
    "- VSLAM is VO with loop detection and closure\n",
    "\n",
    "**3. Describe the needed assumptions for VO.**\n",
    "- **Sufficient illumination** in the environment\n",
    "- **Dominance of static** scene over moving objects\n",
    "- **Enough texture** to allow apparent motion to be extracted\n",
    "- Sufficient **scene overlap** between consecutive frames\n",
    "\n",
    "**4. Illustrate its building blocks**\n",
    "- Image capturing (sequential)\n",
    "- Feature detection & decription (SIFT)\n",
    "- Feature matching (description matching e.g SSD (robust error minimizing))\n",
    "- motion estimation (2D-2D (8-point RANSAC) then 3D-2D (P3+1P) (3D-3D (minimizing euclidean distance))\n",
    "- local optmization (bundle adjustment & pose graph opt.)\n",
    "- global optimization (SLAM) loop closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-03 – Image Formation\n",
    "\n",
    "**1. Explain what a blur circle is**\n",
    "- “Circle of Confusion” or “Blur Circle”\n",
    "- For a fixed film distance from the lens, there is a specific distance between the object and the lens, at which the object appears “in focus” in the image, other points project to a “blur circle” in the image.\n",
    "- R = (Ld/2e) where d is distance between focal plane and image plane\n",
    "\n",
    "**2. Derive the thin lens equation and perform the pinhole approximation**\n",
    "- B/A = e/z\n",
    "- B/A = (e-f)/f = e/f -1\n",
    "- => 1/f = 1/z + 1/e (thin lens eq.)\n",
    "\n",
    "- f ~= e\n",
    "- B/A = f/z\n",
    "- => B = (f/z)A \n",
    "\n",
    "**3. Define vanishing points and lines**\n",
    "- Parallel lines in the world intersect in the image at a “vanishing point”\n",
    "- Parallel planes in the world intersect in the image at a “vanishing line”\n",
    "\n",
    "**4. Prove that parallel lines intersect at vanishing points**\n",
    "- x = (f/Z)X\n",
    "- y = (f/Z)Y\n",
    "- L<sub>1</sub> = P<sub>1</sub> + s* [l, m, n]<sup>T</sup>\n",
    "- L<sub>2</sub> = P<sub>2</sub> + s* [l, m, n]<sup>T</sup>\n",
    "- for s->inf x<sub>VP</sub> = limes f*((X+sl)/(Z+sn)) = f(l/n)\n",
    "- for s->inf y<sub>VP</sub> = limes f*((Y+sm)/(Z+sn)) = f(m/n)\n",
    "\n",
    "**5. Explain how to build an Ames room**\n",
    "- walls not parallel rather trapezoid\n",
    "- brain makes assumtion that was are parallel (perpendicular)\n",
    "\n",
    "**6. Derive a relation between the field of view and the focal length**\n",
    "- tanges is opposing katete over next katete\n",
    "- tan(FoV/2) = (W/2)/f \n",
    "- W is lens hieght\n",
    "\n",
    "**7. Explain and write the equations of the perspective projection, including lens distortion and world\n",
    "to camera projection.**\n",
    "![...](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_3/2_world_to_camera_coordinates.png)\n",
    "![...](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_3/5_world_to_pixel_coordinates.png)\n",
    "![Distortion equation, p. 62](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_3/distortion.png)\n",
    "\n",
    "**8. Given an image and the associated camera pose, how would you superimpose a virtual object on\n",
    "the image (for example, a virtual cube). Describe the steps involved.**\n",
    "- virtual object mesh defined by vertecies -> bring to camera frame using R and t\n",
    "- bring into image frame using the intrinsics\n",
    "- normalize z-axis (lambda) -> (u,v)\n",
    "- distort (virtual object)\n",
    "- draw the object\n",
    "\n",
    "**9. Normalized image coordinates and geometric explanation.**\n",
    "- noramlitzed imagfe coordinates applies the inverse of K to normalize focal length to 1.\n",
    "- triangle normalized f to one.\n",
    "\n",
    "**10. Describe the general PnP problem and derive the behavior of its solutions. What’s the minimum\n",
    "number of points and what are the degenerate configurations?**\n",
    "- in PnP we have the 3D points and their images given and want to find pose of camera\n",
    "- for 1 point we have infinite solutions \n",
    "- for 2 bounded infinity (spidel torus)\n",
    "- for 3 we get 3 independent equations with 3 unknows in total -> product of degrees of equations = 2*2*2 = 8\n",
    "\\begin{align*}\n",
    "{s_i}^2={A_i}^2+{B_i}^2 - 2A_iB_icos(\\theta_i)\n",
    "\\end{align*}\n",
    "- 4 positive ad 4 negative solution. Only positives are reasonable\n",
    "- => 3+1 points to make system overdetermined and disambiguate the 4 solutions\n",
    "-  degenerate configs: at least 2 points colinear with camera center (not on ray), no 3 colinear\n",
    "\n",
    "**11. Explain the working principle of the P3P algorithm. What are the algebraic trigonometric equations\n",
    "that it attempts to solve?**\n",
    "- equation above in 10.\n",
    "- ...\n",
    "\n",
    "**12. Explain and derive the DLT for both 3D objects or planar grids. What is the minimum number of\n",
    "point correspondences it requires (for 3D objects and for planar grids)?**\n",
    "![Matrix equation](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_4/4_matrix_equation.png)\n",
    "![Matrix equation part2](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_4/5_matrix_equation_part2.png)\n",
    "![Matrix equation part3](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_4/6_matrix_equation_part3.png)\n",
    "*Figure 3: Q matrix. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/03_image_formation_2.pdf)*\n",
    "- solve with SVD, m-vector = smallest eigenvector, decompose M into R and t (QR)\n",
    "- for planar grids just ignore third coordinates\n",
    "- 3D: 6 points (non-coplanar and not on a twisted cubic and not 2 colinear with center of projection)\n",
    "- planar: minimum of 4 non-collinear points is required\n",
    "- \n",
    "**13. Define central and non-central omnidirectional cameras.**\n",
    "-  non central onmi-dir-cams have mirros where the rays do not intersec in one point (heind the mirror)\n",
    "**14. What kind of mirrors ensure central projection.**\n",
    "- Mirror must be surface of revolution of a conic =>\n",
    "- Hyperbola + Perspective camera\n",
    "- Parabola + Orthographic lens\n",
    "**15. What do we mean by normalized image coordinates on the unit sphere?**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 – Filtering and edge detection\n",
    "**1. Explain the differences between convolution and correlation**\n",
    "- convolution flips filter first and is assiciative and commutative\n",
    "\n",
    "**2. Explain the differences between a box filter and a Gaussian filter**\n",
    "- box values all pixel with same weight\n",
    "- gaussian weights central pixels more\n",
    "\n",
    "**3. Explain why one should increase the size of the kernel of a Gaussian filter if 2𝜎 is close to the size\n",
    "of the kernel**\n",
    "- at the border the filter should be close to zero otherwise allising effects\n",
    "\n",
    "**4. Explain when we would need a median & bilateral filter**\n",
    "- denoising and preserves strong edges\n",
    "- bilateral preserves all edges up to a threshold\n",
    "\n",
    "**5. Explain how to handle boundary issues**\n",
    "- zero padding (black)\n",
    "- wrap around\n",
    "- copy edge\n",
    "- reflect across edge\n",
    "\n",
    "**6. Explain the working principle of edge detection with a 1𝐷 signal**\n",
    "- smoothing -> gaussian\n",
    "- detection -> derivative\n",
    "- combined ==> derivative of gaussian ==> maximum/minimum is edge\n",
    "- laplacian of gaussian ==> zero crossing is edge\n",
    "\n",
    "**7. Explain how noise does affect this procedure**\n",
    "- whith noise the derivative is always high\n",
    "\n",
    "**8. Explain the differential property of convolution**\n",
    "- instead of using two convolutions One for gaussian and one for derivative use derivative of filter as new filter\n",
    "\n",
    "**9. Show how to compute the first derivative of an image intensity function along 𝑥 and 𝑦**\n",
    "- -1|1 filter, sobel or prewitt\n",
    "\n",
    "**10. Explain why the Laplacian of Gaussian operator is useful**\n",
    "- laplacian of gaussian ==> zero crossing is edge\n",
    "- ==> more accurate/simple localization of edge\n",
    "\n",
    "**11. List the properties of smoothing and derivative filters**\n",
    "Smoothing filter:\n",
    "– has all positive values in filter\n",
    "– sums to 1  preserve brightness of constant regions\n",
    "– removes “high-frequency” components: “low-pass” filter\n",
    "\n",
    "Derivative filter:\n",
    "– has opposite signs used to get high response in regions of high\n",
    "contrast\n",
    "– sums to 0 no response in constant regions\n",
    "– highlights “high-frequency” components: “high-pass” filter\n",
    "\n",
    "**12. Illustrate the Canny edge detection algorithm**\n",
    "- derivative of gaussian\n",
    "- threshold to remove low values\n",
    "- local maxima along gradient\n",
    "\n",
    "**13. Explain what non-maxima suppression is and how it is implemented**\n",
    "- Thinning: non-maxima suppression (local-maxima detection) along gradient direction\n",
    "- each edge we have x and y derivative, combining them gives direction of gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  05-06 – Point feature detection, descriptor, and matching\n",
    "**1. Explain what is template matching and how it is implemented**\n",
    "- move template over image and calculate correlation. high correlation mean good match\n",
    "- different types of correlations can be used\n",
    "- output correlation map\n",
    "\n",
    "**2. Explain what are the limitations of template matching. Can you use it to recognize cars.**\n",
    "- can only find pixel accurate images of the template\n",
    "- cars not pixel accurate\n",
    "\n",
    "**3. Illustrate the similarity metrics SSD, SAD, NCC, and Census transform**\n",
    "- NCC: normalized cross-correlation: cosinus of angle between the normalized vectors\n",
    "- SSD: sum of squared differences\n",
    "- SAD: sum of abosulte differences\n",
    "- census transform: 8 pixel around central pixel if larger then 1 else 0. create bitencoding (append left), start top left, humming distance\n",
    "\n",
    "**4. What is the intuitive explanation behind SSD and NCC**\n",
    "NCC\n",
    "- perpendicular => 0 \n",
    "- opposite => -1\n",
    "- same => 1\n",
    "SSD\n",
    "- larger the error -> large value\n",
    "- best value is 0\n",
    "\n",
    "**5. Explain what are good features to track. In particular, can you explain what are corners and blobs\n",
    "together with their pros and cons. How is their localization accuracy?**\n",
    "- corners are good, edges are medium, falt region are bad, also good are blobs which are any other image pattern that is not a corner and differs significantly from its neighbors\n",
    "- corners are regions with high contrast in at least 2 directions\n",
    "- corners hace hight localizations accuracy but are less distinctive\n",
    "- blobs have lower localizations acc. but are more distinctive.\n",
    "- coners better fpor VO\n",
    "- blobs better for place recoginition\n",
    "\n",
    "\n",
    "**6. Explain the Harris corner detector. In particular:**\n",
    "**- a. Use the Moravec definition of corner, edge and flat region.**\n",
    "- sliding window into different directions and calculate intensity change\n",
    "- all directione no intensity change => flat, change in only one direction => edge, change in at least 2 directions => corner\n",
    "**- b. Show how to get the second moment matrix from the definition of SSD and first order\n",
    "approximation (show that this is a quadratic expression) and what is the intrinsic\n",
    "interpretation of the second moment matrix using an ellipse?**\n",
    "- M = [[I_x^2, I_xI_y][I_xI_y, I_y^2]]\n",
    "- SSD = sum (I(x,y)-I(x+dx, y+dy))^2 ~= sum (I(x,y)- I(x,y) -I_x(x,y)dx-I_y(x,y)dy)^2\n",
    "- shorter axis of ellipse shows faster change in intensity. \n",
    "- langer axis is therefore along edge\n",
    "- if both agis are large => flat\n",
    "\n",
    "**- c. What is the M matrix like for an edge, for a flat region, for an axis-aligned 90-degree corner\n",
    "and for a non-axis—aligned 90-degree corner?**\n",
    "- flat: both eigencvalues small\n",
    "- edge: one eigenvalue large\n",
    "- corner: both eigenvlaues large\n",
    "- axis alignment does not matter since eigenvectors give direction.\n",
    "\n",
    "**- d. What do the eigenvalues of M reveal?**\n",
    "- magnitude of change in intensitiy into direction of eigenvector\n",
    "\n",
    "**- e. Can you compare Harris detection with Shi-Tomasi detection?**\n",
    "- Harris: R = det(M)-k * trace(M)^2\n",
    "- shi-tomasi: R = min(lambda_1, lambda_2)\n",
    "- harris bit cheapter since eigenvlaues do not need to be calculated explicitely, but needs magic number\n",
    "\n",
    "**- f. Can you explain whether the Harris detector is invariant to illumination or scale changes?\n",
    "Is it invariant to view point changes?**\n",
    "- for constant illumination change (affine) ok\n",
    "- cant handle scale changes -> corners become edges when upscaling\n",
    "- view point changes ok as long as corner stays corner\n",
    "**- g. What is the repeatability of the Harris detector after rescaling by a factor of 2?**\n",
    "- cant handle scale changes -> corners become edges when upscaling\n",
    "\n",
    "**1. How does automatic scale selection work?**\n",
    "- take different patch sizes and get cornerness for images independent the size with max value\n",
    "\n",
    "- Sharp, local intensity changes are good regions to monitor in order to identify the scale\n",
    "- The ideal function for determining the scale is one that highlights sharp discontinuities\n",
    "- Solution: convolve image with a kernel that highlights edges (LoG)\n",
    "- Correct scale is found as local maxima or minima across consecutive smoothed images\n",
    "\n",
    "**2. What are the good and the bad properties that a function for automatic scale selection should\n",
    "have or not have?**\n",
    "- A “good” function for scale detection should have a single & sharp peak\n",
    "- The ideal function for determining the scale is one that highlights sharp discontinuities\n",
    "- multiple is bad for the matching then\n",
    "\n",
    "**3. How can we implement scale invariant detection efficiently? (show that we can do this by\n",
    "resampling the image vs rescaling the kernel).**\n",
    "- SIFT\n",
    "- instead of taking larger filters downsampling the image which improves rundtime since larger filter need more resource and making image smaller reduces number of positions (pixels)\n",
    "\n",
    "**4. What is a feature descriptor? (patch of intensity value vs histogram of oriented gradients). How do\n",
    "we match descriptors?**\n",
    "- unique representatgion of a feature which is recognizable\n",
    "- easiest descriptor is patch itself\n",
    "- calculated de difference and match the ones with lowest error\n",
    "- more robust is also to only match when best match is mcuh better than second best match (0.8)\n",
    "\n",
    "**5. How is the keypoint detection done in SIFT and how does this differ from Harris?**\n",
    "- octave of gaussians filtered images with different sigmas\n",
    "- taking differences of these gaussians DoG\n",
    "- take 3x3x3 box in layered DoGs, if central pixel is largest and over threshold then we found a keypoint\n",
    "- SIFT ist faster since it downsample therefre can fastly try big range of sigmas\n",
    "\n",
    "**6. How does SIFT achieve orientation invariance?**\n",
    "- SIFT secriptor is orientation invarian since the HoG of the cells is shift so that over all larges value is first\n",
    "\n",
    "**7. How is the SIFT descriptor built?**\n",
    "- 16x16 patch\n",
    "- gauss on whole patch\n",
    "- 4x4 cells\n",
    "- HoG for each cell\n",
    "- concatenate all HoGs\n",
    "- shift\n",
    "\n",
    "**8. What is the repeatability of the SIFT detector after a rescaling of 2? And for a 50 degrees’ viewpoint\n",
    "change?**\n",
    "- 50 degree viewpoint chage is upper limit for which it works reasonably\n",
    "- 50 degree viewpoint chage 50% correct matches\n",
    "- scaling change of 2: scaleinvariant (if enought octaves)\n",
    "\n",
    "**9. Illustrate the 1st to 2nd closest ratio of SIFT detection: what’s the intuitive reasoning behind it?\n",
    "Where does the 0.8 factor come from?**\n",
    "- d1/d2 < 0.8\n",
    "\n",
    "**10. How does the FAST detector work? What are its pros and cons compared with Harris?**\n",
    "- ring with usually 16 pxl around centerpixel then see how many consecutive pixels are bighter that center pixel \n",
    "- if 9 or more consecutive pixel are all bighter of darker then feature is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 – Stereo Vision\n",
    "**1. Can you relate Structure from Motion to 3D reconstruction? What’s their difference?**\n",
    "- SFM creates sparse scene point cloud, and gives the camera positions\n",
    "- dense 3D reconstruction needs camera poses given and then creates a dense map\n",
    "- SFM non sequential, dense recon in shown case sequential\n",
    "- SFM based on features\n",
    "- dense reconstruction based on pixels\n",
    "\n",
    "- 3D reconstruction: K, R, t are know, Goal: 3D structure\n",
    "- SFM: none known, Goal: 3D structure and K, R and t\n",
    "\n",
    "**2. Can you define disparity in both the simplified and the general case?**\n",
    "- simplyfied: pixel displacement along main axis- since recified\n",
    "- general: displcement along epipolar line\n",
    "\n",
    "**3. Can you provide a mathematical expression of depth as a function of the baseline, the disparity\n",
    "and the focal length?**\n",
    "- depth = (baseline * focal length)/ disparity (Only in simplified case)\n",
    "\n",
    "**4. Can you apply error propagation to derive an expression for depth uncertainty and express it as a\n",
    "function of depth? How can we improve the uncertainty?**\n",
    "- uncertaity depending on disparity : Delta-depth = abs(derivative of depth-function by disparity) delta-disparity\n",
    "- ==> delta-depth = (b * f)/(disparity^2) delta-disp\n",
    "- ==> delta-depth = (depth^2/(b * f)) delta-disp\n",
    "- improfe by larger baseline, better resolution, larger f\n",
    "\n",
    "**5. Can you analyze the effects of a large/small baseline? Can you illustrate it with a sketch?**\n",
    "- see formula from above\n",
    "![Large and Small Baseline](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_8/5_large_and_small_baseline.png)\n",
    "\n",
    "**6. What is the closest depth that a stereo camera can measure?**\n",
    "- bf/max-disparity \n",
    "- max-disp = width-left-camera/2 - (-width-right-camera/2) = width-left-camera + width-right-camera\n",
    "- for identic cameras: width cameras\n",
    "\n",
    "**7. Are you able to show mathematically how to compute the intersection of two lines (linearly and\n",
    "non-linearly)?**\n",
    "- linear: lambda * p_1 = M_1 * P, due to this equaility we knwo these vectors are paralell\n",
    "- ==> parallel vectors have a cross-product of 0\n",
    "- ==> p_1 x M_1 * P = 0 same for other camera\n",
    "- ==> p_2 x M_2 * P = 0\n",
    "- solve this system using svd\n",
    "\n",
    "- non linear: initialize with linear\n",
    "- then optimize using error minimization (SSRE (reprojection))\n",
    "\n",
    "**8. What is the geometric interpretation of the linear and non-linear approaches and what error do\n",
    "they minimize?**\n",
    "- non-linear minimizes SSRE\n",
    "- linear: midpoint of the segment connecting the two backprojected rays (minimized distance from both rays)\n",
    "\n",
    "**9. Are you able to provide a definition of epipole, epipolar line and epipolar plane?**\n",
    "- epipoles: intertsection of line connection both camera-centers with their cameraplanes\n",
    "- epipolar-line intersection of epipolarplane with camera-plane (backprojection of ray onto other cameras cameraplane)\n",
    "- epipolarplane: plane span by camera-centers and world-point\n",
    "\n",
    "**10. Are you able to draw the epipolar lines for two converging cameras, for a forward motion situation,\n",
    "and for a side-moving camera?**\n",
    "![Examples](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_8/13_epipolar_examples.png)\n",
    "\n",
    "**11. Are you able to define stereo rectification and to derive mathematically the rectifying\n",
    "homographies?**\n",
    "- warp of the image to how it would like like if the cameras would be perfectly aligned\n",
    "- apply camera specific rotation ans intrinsics inversed to get normalized representation to both views (not displayable)\n",
    "- apply \"mean\" rotation and intrisict to both views\n",
    "- K-hat = average of both K\n",
    "- R-hat: r_hat_1 = normalized vector between camera-centers (C_2-C_1)/norm(C_2-C_1)\n",
    "- r_hat_2 = r_L_3 x r_hat_1\n",
    "- r_hat_3 = r_hat_1 x r_hat_2\n",
    "\n",
    "**12. How is the disparity map computed?**\n",
    "- recitify images\n",
    "- match points along epipolar line (using similarity like cross-correlation)\n",
    "- measure disparity\n",
    "- done, map\n",
    "\n",
    "**13. How can we establish stereo correspondences with subpixel accuracy?**\n",
    "- interpolate cross-correlation function\n",
    "\n",
    "**14. Describe one or more simple ways to reject outliers in stereo correspondences.**\n",
    "- take diretion of movement into account, e.g stereo case left right image -> from left to right the correspondance should move to the left\n",
    "- bigger patch\n",
    "\n",
    "**15. Is stereo vision the only way of estimating depth information? If not, are you able to list alternative\n",
    "options? (make link to other lectures of course)**\n",
    "- no\n",
    "- n-view-SFM, 2-view-SFM, dense-3d-reconstruction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  08-09– Multiple view geometry 2 and 3\n",
    "\n",
    "**1. What's the minimum number of correspondences required for calibrated SFM and why?**\n",
    "- 5, because we have 4n knowns for n correspondences (u,v for both views) and we have 5 unknows for the motion (3 for rotation, 2 for translation because of scale ambiguity we normalize for one of the coordinates, and we also have 3 unknoes for eacht correspondance (world-point)\n",
    "- to solve system we need more/at-least knowns than unknows\n",
    "- 4n >= 5+3n ==> n >= 5\n",
    "\n",
    "**2. Are you able to derive the epipolar constraint?**\n",
    "- p_1 dot n must be 0 because they need to be perpendicular because of definition of normal vector of epipolarplane and p_1 has to be on epipolar plane\n",
    "- then we express n as Tx(R * p_2), we need T for the translation and R for the roation betwenn the camera frames\n",
    "- ==> T_x (translation as cross-product-matrix) * R = E (essential matrix)\n",
    "- epipolar constraint: p_2^T * E * p_1 = 0\n",
    "\n",
    "**3. Are you able to define the essential matrix?**\n",
    "- Yes\n",
    "\n",
    "**4. Are you able to derive the 8-point algorithm?**\n",
    "- p_i is expressed as (u_i, v_i, 1) \n",
    "- Write the epipolar constrain as sytem of equation dependen on unknow elements of E\n",
    "- Write the system as matrix multiplication where we express E as a vector (Q * E = 0)\n",
    "- Q is the vertical stack of multiple of such equations for different correspondances  \n",
    "- solve this system using SVD\n",
    "- E is the eifenvector to the smallest eigenvalue\n",
    "- then we decompose E using SVD\n",
    "- extract R und T according to slide which wont be asked\n",
    "\n",
    "- (normaization)\n",
    "**5. How many rotation-translation combinations can the essential matrix be decomposed into?**\n",
    "- into 4 but only one of then poses the point in front of the cameras\n",
    "\n",
    "**6. Are you able to provide a geometrical interpretation of the epipolar constraint?**\n",
    "![Epipolar Constraint](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_9/2_epipolar_constraint.png)\n",
    "\n",
    "**7. Are you able to describe the relation between the essential and the fundamental matrix?**\n",
    "- fundamental matrix is where we don't know the intrinsics so it contains them.\n",
    "- F = K_2^(-T) * E * K_1^(-1)\n",
    "\n",
    "**8. Why is it important to normalize the point coordinates in the 8-point algorithm?**\n",
    "- because otherwise Q can be poorly conditioned\n",
    "- since the values are from 0-Xwhen multipling the valuesw can get very big.\n",
    "- only problems when solving the numerical solution, multiplies the noise\n",
    "\n",
    "**9. Describe one or more possible ways to achieve this normalization.**\n",
    "- move orgin to centerpoint and normalize coordinates to [-1,1]\n",
    "- normalize mean to be the (0,0) and normalize the standart deviation to sqrt(2) \n",
    "- Normalization can be expressed as matrix B: p_norm = Bi * pi\n",
    "![Hartley Scaling](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_9/10_hartley_scaling.png)\n",
    "\n",
    "**10. Are you able to describe the normalized 8-point algorithm?**\n",
    "- normalize point correspondances as described above (pi_norm = Bi * pi)\n",
    "- estimate normalized F_norm with normalized coordinates\n",
    "- compute unnormalized F = B2^T * F_norm * B1\n",
    "\n",
    "**11. Are you able to provide quality metrics for the essential matrix estimation?**\n",
    "- algebraic error (deviation from 0 wen taking cross product)\n",
    "- directional error (normalize algebraic)\n",
    "- epipolarline distance (distance from image to epipolar line)\n",
    "- reprojection error\n",
    "\n",
    "**12. Why do we need RANSAC?**\n",
    "- to remove outliers\n",
    "\n",
    "**13. What is the theoretical maximum number of combinations to explore?**\n",
    "- for pairs: n(n-1)/2\n",
    "- for s points in sample: n_choose_s = n!/(s!(n-s)!)\n",
    "\n",
    "**14. After how many iterations can RANSAC be stopped to guarantee a given success probability?**\n",
    "- k = log(1-p)/log(1-w^s) where p is probability of success, w is fraction of inliers,  s in number of points in sample\n",
    "\n",
    "**15. What is the trend of RANSAC vs. iterations, vs. the fraction of outliers, vs. the number of points to\n",
    "estimate the model?**\n",
    "- exponential curves, where 8-point rises first, then 5 then 2\n",
    "\n",
    "**16. How do we apply RANSAC to the 8-point algorithm, DLT, P3P?**\n",
    "- we use 8 points to estimate model using 8-point algo and look how many inliers and the repeade k times\n",
    "- we use 6 points to estimate model using DLT algo and look how many inliers and the repeade k times\n",
    "- we use 3+1 points to estimate model using P3P algo and look how many inliers and the repeade k times\n",
    "\n",
    "**17. How can we reduce the number of RANSAC iterations for the SFM problem? (1- and 2-point\n",
    "RANSAC)**\n",
    "- movement constraints\n",
    "- planar -> 2 points\n",
    "- planar circular -> 1 point\n",
    "\n",
    "**18. Bundle Adjustment and Pose Graph Optimization. Mathematical expressions and illustrations. Pros\n",
    "and cons.**\n",
    "- BA: minimzes reprojection errors and adjusts camera-poses and 3D-points\n",
    "\\begin{align*}\n",
    "(P^i,C_2) = argmin_{P^i,C_1,C_2} \\sum_{i=1}^{N} \\lVert {p_1}^i - \\pi_1(P^i,C_1)\\rVert^2 + \\lVert {p_2}^i - \\pi_2(P^i,C_2)\\rVert^2\n",
    "\\end{align*}\n",
    "\n",
    "- PGO: minizes pose-error (roto-translations) for all possible poses that have same correspondences, only adjusts poses\n",
    "\\begin{align*}\n",
    "C_k = argmin_{C_k} \\sum_{i} \\sum_{j} \\lVert C_i - C_jT_{ij} \\rVert^2\n",
    "\\end{align*}\n",
    "\n",
    "**19. Are you able to describe hierarchical and sequential SFM for monocular VO?**\n",
    "hierarchical: \n",
    "- clusters of 3 view with same features\n",
    "- construct point cloude from two of these (2D-2D)\n",
    "- merge third view (3D-2D)\n",
    "- merge these clusters (3D-3D)\n",
    "\n",
    "sequential\n",
    "- bootstrap (2D-2D for first two keyframes) -> pointcloud\n",
    "- then until next keayframe just get pose (P3P)\n",
    "- new keyframe add more points to pointclouds (P3P for pose then triangulation for 3D-points)\n",
    "- new keyframe when key-distance/avg-scene-depth = 0.1 to 0.2\n",
    "\n",
    "- pro/con expensive for to many keyframes and to small baselines -> to big of a depth uncertaity, too few keyframes -> to few correspondances\n",
    "\n",
    "**20. What are keyframes? Why do we need them and how can we select them?**\n",
    "- see above\n",
    "\n",
    "**21. Are you able to define loop closure detection? Why do we need loops?**\n",
    "- loop detection recognizin that we are at a place we were before (placerecognition)\n",
    "- then we can try to close the loop because we know it should be but our estimation sais different thing (pose-graph-otimization)\n",
    "- ==> VSLAM\n",
    "- we need loops to know that we are drifting\n",
    "- we need loops for loop closure which improves our optimizations (remove drifts)\n",
    "\n",
    "**22. Are you able to provide a list of the most popular open source VO and VSLAM algorithms?**\n",
    "- ORB-SLAM\n",
    "- LSD-SLAM\n",
    "- DSO\n",
    "- SVO\n",
    "\n",
    "**23. Are you able to describe the differences between feature-based methods and direct methods?**\n",
    "- feature based methods are based on features not pixels\n",
    "-  direct methods are based on pixel intensities\n",
    "\n",
    "**24. Sparse vs semi-dense vs dense. What are their pros and cons?**\n",
    "- for spares methods we only look ar some view pixels\n",
    "- semi-dense: we look ant more pixels\n",
    "- dense: we look at all pixels\n",
    "\n",
    "- dense methods are more costly because we have to calculate for all pixels buit tend to be more accurate\n",
    "- sparse are faster, less accurate\n",
    "- dense are useful with motion blure and defocus\n",
    "- sparse are equal good to dense when overlap is >30%\n",
    "\n",
    "**25. General definition of VO and comparison with respect VSLAM and SFM. Definition of loop closure\n",
    "detection (why do we need loops?). How can we detect loop closures? (make link to other\n",
    "lectures).**\n",
    "- SFM > VSLAM > VO\n",
    "- loops see above\n",
    "\n",
    "##  10 – Multiple view geometry 4 (benchmarking visual SLAM) not covered in class, so it won’t be asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  11 – Tracking\n",
    "\n",
    "**1. Are you able to illustrate tracking with block matching?**\n",
    "- ...\n",
    "**2. Are you able to explain the underlying assumptions behind differential methods, derive their\n",
    "mathematical expression and the meaning of the M matrix?**\n",
    "- ...\n",
    "**3. When is this matrix invertible and when not?**\n",
    "- ...\n",
    "**4. What is the aperture problem and how can we overcome it?**\n",
    "- ...\n",
    "**5. What is optical flow?**\n",
    "- ...\n",
    "**6. Can you list pros and cons of block-based vs. differential methods for tracking?**\n",
    "- ...\n",
    "**7. Are you able to describe the working principle of KLT?**\n",
    "- ...\n",
    "**8. What functional does KLT minimize? (proof won’t be asked, only the first two slides titled\n",
    "“derivation of the Lucas-Kanade algorithm”)**\n",
    "- ...\n",
    "**9. What is the Hessian matrix and for which warping function does it coincide to that used for point\n",
    "tracking?**\n",
    "- ...\n",
    "**10. Can you list Lukas-Kanade failure cases and how to overcome them?**\n",
    "- ...\n",
    "**11. How do we get the initial guess?**\n",
    "- ...\n",
    "**12. Illustrate alternative tracking using point features.**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12a – Dense 3D Reconstruction\n",
    "\n",
    "**1. Are you able to describe the multi-view stereo working principle? (aggregated photometric error)**\n",
    "- ...\n",
    "**2. What are the differences in the behavior of the aggregated photometric error for corners, flat\n",
    "regions, and edges?**\n",
    "- ...\n",
    "**3. What is the disparity space image (DSI) and how is it built in practice?**\n",
    "- ...\n",
    "**4. How do we extract the depth from the DSI?**\n",
    "- ...\n",
    "**5. How do we enforce smoothness (regularization) and how do we incorporate depth discontinuities\n",
    "(mathematical expressions)?**\n",
    "- ...\n",
    "**6. What happens if we increase lambda (the regularization term)? What if lambda is 0? And if lambda\n",
    "is too big?**\n",
    "- ...\n",
    "**7. What is the optimal baseline for multi-view stereo?**\n",
    "- ...\n",
    "**8. What are the advantages of GPUs?**\n",
    "- ...\n",
    "\n",
    "## 12b – Place Recognition\n",
    "\n",
    "**1. What is an inverted file index?**\n",
    "- ...\n",
    "**2. What is a visual word?**\n",
    "- ...\n",
    "**3. How does K-means clustering work?**\n",
    "- ...\n",
    "**4. Why do we need hierarchical clustering?**\n",
    "- ...\n",
    "**5. Explain and illustrate image retrieval using Bag of Words.**\n",
    "- ...\n",
    "**6. Discussion on place recognition: what are the open challenges and what solutions have been\n",
    "proposed?**\n",
    "- ...\n",
    "\n",
    "## 12c – Deep Learning – Won’t be asked at the exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 – Visual inertial fusion\n",
    "\n",
    "**1. Are you able to answer the following questions?**\n",
    "- ...\n",
    "**2. Why is it recommended to use an IMU for Visual Odometry?**\n",
    "- ...\n",
    "**3. Why not just an IMU, without a camera?**\n",
    "- ...\n",
    "**4. How does a MEMS IMU work?**\n",
    "- ...\n",
    "**5. What is the drift of an industrial IMU?**\n",
    "- ...\n",
    "**6. What is the IMU measurement model?**\n",
    "- ...\n",
    "**7. What causes the bias in an IMU?**\n",
    "- ...\n",
    "**8. How do we model the bias?**\n",
    "- ...\n",
    "**9. How do we integrate the acceleration to get the position (formula)?**\n",
    "- ...\n",
    "**10. What is the definition of loosely coupled and tightly coupled visual inertial fusions?**\n",
    "- ...\n",
    "**11. How can we use non-linear optimization-based approaches to solve for visual inertial fusion?**\n",
    "- ...\n",
    "**12. Can you write down the cost function of smoothing methods and illustrate its meaning?**\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 – Event-based Vision\n",
    "\n",
    "**1. Are you able to answer the following questions?**\n",
    "- ...\n",
    "**2. What is a DVS and how does it work?**\n",
    "- ...\n",
    "**3. What are its pros and cons vs. standard cameras?**\n",
    "- ...\n",
    "**4. Can we apply standard camera calibration techniques?**\n",
    "- ...\n",
    "**5. How can we compute optical flow with a DVS?**\n",
    "- ...\n",
    "**6. Could you intuitively explain why we can reconstruct the intensity?**\n",
    "- ...\n",
    "**7. What is the generative model of a DVS (formula)? Can you derive its 1st order approximation?**\n",
    "- ...\n",
    "**8. What is a DAVIS sensor?**\n",
    "- ...\n",
    "**9. What is the focus maximization framework and how does it work? What is its advantage\n",
    "compared with the generative model?**\n",
    "- ...\n",
    "**10. How can we get color events?**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
