{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Chapter 2 - Optics\n\nIn this article, we are goint to find out how an image is formed on an Image Plane. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The overall principle is quite basic: An object reflects light that falls on an Image sensor, which captures the lights intensity and\ntherefore forms an image. To ensure that every part of the scenery only falls onto the optical sensor at one spot only, we can introduce\nan optical barrier with a hole in it which ensures that - for each point in our scene - only light rays with a particular \nangle fall onto the image plane.\nWe can therefore create an upside down copy of the scenery on our optical sensor. The smaller the barriers hole, the more angle-selective\nour camera becomes, the sharper the image appears. The hole is also known as aperture, or pinhole. \n\n![Pinhole Model, p. 7]()\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "An ideal pinhole camera has - mathematically - an infinitely small pinhole to have an infinitely sharp image. In practice, this comes in\nhand with two problems: The smaller the pinhole, the less light we can capture on our sensor. Furhter, a pinhole smaller than 0.3mm will\ncause lightwave interferreneces, making the image blurry again due to diffraction effects. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To combat these issues, lenses are used. They have the property that they bundle lightrays coming from the same point in our scenery\ninto a (preferrably) single spot on the optical sensor. Lenses must fullfill two characteristics to be fitting camera lenses:\n- 1. Light rays that travel through the optical center of the lens will not be deviated\n- 2. Lightrays that fall parallel to the optical axis into the lens are focues in a so called \"focal point\" f behind the lens. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "With combining these two properties, we can derrie the \"thin lense equation\": From a single Object in our Scene at distanze Z and height A\nin front of our lens, we can construct two lighrays: One passing through the optical center, the other entering the lens parallel\nto the optical axis. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To make the Object appear sharp, we have to ensure that both lighrays fall onto the same point on our optical sensor. Since the\nfocal point is given by the lens properties, we can not vary the focal length *f* (distance between focal point and lens). What we \ncan change is the position of the optical sensor: We can either bring it closer or farther to lens. We call the distance to the\noptical sensor *e*. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "With looking back at similarity triangles principles, we can see that The objects elevation *A* in respect to the position of the \nsensor crossing point *B* must be the same as the distance to the Object *z* in respect to the distance from the Lens to the \nImage Sensor *e*. We therefore conduct that *B/A \u003d e/z*. \nAs a second equation, we can conduct that *B/A* must also be equal to the ration *e-f / f*. To simplify, we can also write\n*B/A \u003d e/f - 1*.\n\n![Thin Lens equation, p. 17]()\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "With combining these two equations, we get that *e/f - 1 \u003d e/z* and finally **1/f \u003d 1/z + 1/e**. Therefore, for a given\ndistance *z*, the object only appears sharp if the optical sensor is distance *e* away from the Lens.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "It becomes clear that if we move an Object further away (increasing *z*), the distance e must be changed to make the object appear\nsharp again. \nWhen the thin lens equation is not satisfied, the light rays do not intersect at the optical sensor, creating a blur circle which\nis perceived as \"unsharp\". Only a blur circle with radius less than 1 Pixel gives a sharp image. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The distance between the focal Plane (where the light rays at this distance actually meet) and the image plane (optical sensor)\nis called *S*, the diameter of a Pinhole is referred to as *L*. For simple pinhole cameras, this gives us a blur circle \nradius of **R \u003d LxS / 2xe**. \n\n![Blur circle, p. 19]()\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Why is this relevant? Well, for large distances to our object *z* we can approximate our lens-based camera model with a pinhole camera,\nsince the distance to any world object is much much larger than the focal length or the Lens size. Typically, smartphone cameras\nhave focal lengths of 1.7mm and Lens sizes of \u003c 1 cm. We can therefore focus at objects at infinity. This implies that the \nfocal plane for objects that are inifinitely far away moves to the focal point. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We can therefore safely aproximate that the focal length *f* is equal to the optical sensor distance *e*: *e ~\u003d f*. This makes the \nrelation between our objects elevation *A* and the point on the Image plane *B* even simpler: We don\u0027t have to consider two\nlightrays but just one falling straight throught the pinhole. This also leaves us with a simpler equation to\nfind the point *B* where a object at distce *z* and elevation *A* would fall on the Image sensor: *B/A \u003d f/z*, or *B \u003d f/z x A*. \nTherefore, Objects twice as far away appear half as large in on the optical sensor. \n\n![Pinhole approximation, p. 22]()\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The distance range in which an object appears sharp (R \u003c 1 Pixel) is called the Depth of Field (DoF). The smaller the apperture, \nthe larger the Depth of Field, but the less light we have left for the Image Sensor. \nFurther, the lens size at a fixed focal length defines the biggest Angle the camera can perceive, the Field of View (FoV). \nWe can also increase/decrease the FoV by changing the focal length: Larger focal lengths intuitively result in a more narrow\nviewing angle. The ratio between the focal length and the FoW angle *p* can be simply expressed via a tangential relation:\n*tan(p/2) \u003d W/2 x f*\n\n![Pinhole approximation, p. 22]()\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "An interesting consequence of perspecitve projection is that parallel lines in the 3D world are no longer perceived as parallel on the 2D image\nplane. Neither are angles preserved. Further, it seems that parallel lines in an image will cross at some point, the so called\nvanishing point. With tracking all these vanishing points, we can fit a vanishing line through them: A line on which all vanishing\npoints land. We observe two vanihshing lines: one for horizontal and one for vertical parallel lines. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To understand why this happens, we make a quick excursion to homogeneous coordinates. We are used to having a euclidean coordinate\nsystem with (x,y)-Coordinates. But in a perspective sense, we also have a third dimension, the distance, leaving us with\nthe coordinates (x, y, w). We can translate from the euclidean system to the homogenous one with interpreting the euclidean coordinate\nsystem as a plane on w\u003d1, m aking (x,y) \u003d (x,y,1). \nWe can also map homogenous coordinates back into euclidean ones by defining (x,y) \u003d (x/w, y/w). \nTo represent a point that is infinitely far away, we\u0027d need a point (x,y) \u003d (oo, oo) in the euclidean system. The same point\nrepresented in homogenous coordinates would just be (x,y,0). \nWe can therefore proof that two parallel lines will intersect at infinity. Let\u0027s define two parallel lines: *L1 \u003d Ax + By + C \u003d 0* and \n*L2 \u003d Ax + By + D \u003d 0*. Since C must be different from D (otherwise the lines would be the same), there is no solution for the\neuclidean system. However, in the homogenous form, we have the corresponding line definitions *L1 \u003d Ax/w + By/w + C \u003d 0* and \n*L2 \u003d Ax/w + By/w + D \u003d 0*, so for (x, y, 0) the equation is satisfied. Therefore, two lines meet at some point (x,y,0), which is\nsomewhere at infinity since *w* \u003d 0.\n\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}