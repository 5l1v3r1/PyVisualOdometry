{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 6 - Point Feature Detection\n",
    "\n",
    "In the last blog article, we've seen how filters can be used to find edges in an image. However, we can also use filters to detect features in an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Matching\n",
    "To illustrate this concept, let's have a look at an image with some simple structures on it. We can copy a small region of our image and use it as a filter / template. When we slide this filter over the image, we'll get a so called *correlation map* that indicates how well the template matched the original picture at each possible position. As we might expect, the point of best correlation is found at the brightest pixel at exactly the position where we extracted the template originally. \n",
    "\n",
    "![Template matching](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/template_matching.png)\n",
    "*Figure 1: Template, Detected template and correlation map. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the difference between the template and the image, we can use **cross correlation**, hence the name **cross correlation map**. In normalized cross correlation (NCC), we interpret pixels as vectors and take their cross product divided by the vectors lengths. If the images / vectors are equal, their NCC will be 1. If they are perpendicular, the NCC is 0 - a NCC of -1 suggest that the vectors are exact opposites of each other. \n",
    "While NCC is very robust, the problem with NCC is the computational complexity: It is computationally expensive to calculate NCC, hence it is seldomly used in practice. \n",
    "\n",
    "![Normalized Cross Correlation](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/cross_correlation.png)\n",
    "*Figure 2: Cross correlation formula. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative and much faster similarity measurement is the **Sum of Squared Differences** (SSD). SSD, as the name sais, takes the piecewise difference of each pixel value, squares it and sums the results. \n",
    "\n",
    "![Sum of Squared Differences](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/sum_of_squared_differences.png)\n",
    "*Figure 3: Sum of Squared Differences. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even cheaper than SSD is the third similarity measurement, called **Sum of Absolute Differences** (SAD). It works like SSD but replaces the square with the absolute function, making it even faster to compute but also less robust. \n",
    "\n",
    "![Sum of Absolute Differences](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/sum_of_absolute_differences.png)\n",
    "*Figure 4: Sum of Absolute Differences. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these difference measurements are prone to intensity variations. To account for this, we can substract the average intensity of the two images (typically caused by additive illumination changes) from each image. We then get what's called **Zero-mean Normalized Cross-Corrleation (ZNCC)**, **Zero-mean Sum of Squared Differences ZSSD)** and **Zero-mean Sum of Absolute Differences ZSAD)**. \n",
    "\n",
    "![Zero-mean similarities](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/zero_mean_similarities.png)\n",
    "*Figure 5: Zero-mean similarities. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last error measurement we will have a look in this article is called **Census Transform**. CT maps each pixel in the template patch to either 1 (white) or 0 (black) depending whether it is brighter or darker than the central pixel. Then, it flattens the pixels clockwise into a vector called *bit string*. For a template with dimensions w * w, the resulting bit string vector would be of length w^2 - 1 (minus the central pixel). We can then compare two patches by converting them to a bit string and apply *Hamming distance*, which is the total number of bits that are different. This can be done by performing an XOR operation on both bit strings. \n",
    "This process has the advantage that it is very fast since neither roots nor divisions are required. Since the pixels are compared to the central pixel, the Census Transform is automatically also invariant to monotonic intensity changes. \n",
    "\n",
    "![Census Transform](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/census_transform.png)\n",
    "*Figure 6: Census Transform. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While template matching is cool, it's applications are limited to cases where we match on a pixel level. We can't match objects that are just similar to the pixel. We even fail if the overall brightness of the image changed or the rotation / orientation slightly changed. As we compare exact pixel values, we also need to have the exact pixel equivalent to the filter inside the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-feature extraction\n",
    "For visual odometry, we want to find and extract features from an image that we can use as templates in another image. This way, we can match features between two images and calculate the relative motion between them. \n",
    "\n",
    "Matching keypoints can also be used for other applications like *Panorama Stitching*, *Object recognition*, *3D reconstruction* or *place recognition*. The problem is always the same: Given one image, we want to match another image of the same scene but taken under different environmental conditions. \n",
    "\n",
    "### Panorama stitching\n",
    "As an easy example, let's consider panorama stitching where we have two images taken side by side with a certain overlap, and we want to combine these two images to get a panorama stitch. We can dothis in three steps:\n",
    "- 1.) Find features in both images\n",
    "- 2.) For each feature, find the corresponding pair in the other picture\n",
    "- 3.) Overlap images such that the pairs are aligned as good as possible\n",
    "\n",
    "![Panorama Stitching](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/panorama_stitching.png)\n",
    "*Figure 7: Panorama Stitching. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "The hard part is to detect the same points independently in both images. We need to have repeateable feature detectors, meaning that we have to re-detect the same features from one images in another. \n",
    "Secondly, we have to find the corresponding features in both images distinctively. To do so, we use descriptors. Descriptors are a description of a pixel and the features around it that uniquely identify the pixel without ambiguity. The descriptor needs to be robust against geometric and illumination changes such that we can find the same descriptor in a different image. Such geometrical changes can be: Translation, Rotation, Scaling and perspective changes. \n",
    "\n",
    "An illumination change is a simple, affine transformation of a pixels value by a constant factor *B*. It can easily be overcome by dividing an image by its average intensity value. \n",
    "Rotation invariation is harder to achieve. We could - for example - always rotate our detected features such that the most dominant lines are horizontal, but we will see more efficient methods later in this article. \n",
    "\n",
    "When we detect features, we usually have two different methods: **corner detection** and **blob detection**. \n",
    "A corner is a region of contrast that changes significantly in two perpendicular directions. It has a very high localization accuracy but are often hard to distinct. In contrast, a Blob is an image region that differs significantly from its neighbours - like a small but dominant pattern. Blogs are less localization accurate than corners but are more distinct and therefore easier to redetect. \n",
    "\n",
    "## Moravec Corner Detector\n",
    "Moravec proposed a corner detectoor in which we slide a window into any direction and measure the intensity change. A corner is found when movement into at least two directions results in a large change in intensity. \n",
    "\n",
    "![Moravec Corner Detector](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/moravec_corner_detector.png)\n",
    "*Figure 7: Moravec Corner Detector. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "We consider a window / patch at position (x,y) and move it to a new location (x+dx, y+dy). we then copute the sum of squared differences between the two patches is large for two different dx and dy pairs, we consider the region (x,y) to contain a corner. \n",
    "\n",
    "![SSD of Moravec Corner Detector](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/ssd_moravec_corner_detector.png)\n",
    "*Figure 8: SSD of Moravec Corner Detector. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "While Moravecs approach is intuitive to understand, it is also computationally expensive, since for each pixel location, multiple different patch positions have to be calculated and compared to each other. \n",
    "\n",
    "## Harris Corner Detector\n",
    "Harris implements Moravecs corner detection without the need to physicalls shift the patch window around. It does so by looking at the patch and calculating the gradients, so the derivatives. \n",
    "Harris approximates the shift (dx, dy) using the first taylor expansion: I(x + dx, y + dy) = I(x, y) + Ix(x, y)dx + Iy(x, y)dy. \n",
    "The SSD of (dx, dy) can therefore be approximated by taking the sum of squared differences between the images derivative in x-direction (Ix) and the images derivative in y-direction (Iy).\n",
    "\n",
    "![SSD of Harris Corner Detector](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/ssd_harris_corner_detector.png)\n",
    "*Figure 9: SSD of Harris Corner Detector. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "To implement this formula, we can write the SSD in matrix form. Note that in the following formula, M is a so called *second moment matrix* containing pixel-wise products of their respective image patch. \n",
    "\n",
    "![SSD of Harris in Matrix representation](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/harris_corner_detector_matrix.png)\n",
    "*Figure 9: SSD of Harris in Matrix representation. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
